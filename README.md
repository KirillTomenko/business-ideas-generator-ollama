## Генератор бизнес-идей (VCd01) — Локальная версия на Ollama

Мини-приложение с псевдоагентом для исследования рынка и генерации бизнес-идей, работающее **полностью локально** без доступа к интернету.

- **Backend**: `FastAPI` + `Ollama` (локальная LLM через REST API)
- **Frontend**: минималистичный HTML+CSS+JS (викторианский стиль)
- **LLM**: `Ollama` с моделью `gemma3:1b` (или другой на выбор)

---

## Требования

- **Python 3.11+** (рекомендуется 3.14+)
- **Ollama** установлена и запущена локально
- **Модель Ollama** (например, `gemma3:1b`, `deepseek-r1:8b`)

---

## Установка Ollama и моделей

1. Скачайте Ollama с официального сайта: https://ollama.com/
2. Установите программу
3. Скачайте нужную модель через командную строку:
```bash
# Лёгкая модель (быстрая, ~1 GB)
ollama run gemma3:1b

# Более мощная модель (точнее, ~5 GB)
ollama run deepseek-r1:8b
```

4. **Важно**: В настройках Ollama включите галочку **"Expose Ollama to the network"**
   - Найдите иконку Ollama в трее (скрытые значки)
   - Правый клик → Settings
   - Поставьте галочку "Expose Ollama to the network"

---

## Структура проекта
```
VCd01/
├── backend/
│   ├── main.py              # FastAPI сервер
│   ├── ollama_client.py     # Клиент для Ollama API
│   └── requirements.txt     # Зависимости Python
└── frontend/
    └── index.html           # Веб-интерфейс (викторианский стиль)
```

---

## Установка зависимостей
```bash
cd VCd01
pip install -r backend/requirements.txt
```

**Содержимое `requirements.txt`:**
```txt
fastapi
uvicorn
requests
pydantic
```

---

## Запуск приложения

### Шаг 1: Запустите Ollama с моделью

Откройте **первый терминал** и запустите:
```bash
ollama run gemma3:1b
```

☝️ **Не закрывайте этот терминал!** Ollama должна работать постоянно.

---

### Шаг 2: Запустите Backend

Откройте **второй терминал** и выполните:
```bash
cd C:\Users\SNHIM\Desktop\VCd01
python -m uvicorn backend.main:app --reload
```

Или напрямую:
```bash
cd backend
python main.py
```

После успешного запуска backend будет доступен на `http://127.0.0.1:8000/`

**Эндпоинты:**
- `GET /` — Информация о сервере
- `GET /health` — Проверка статуса Ollama
- `POST /api/generate` — Генерация бизнес-идей

---

### Шаг 3: Откройте Frontend

Перейдите в папку `frontend` и откройте файл `index.html` в браузере:

**Способ 1 (самый простой):**
- Найдите файл `frontend/index.html` в проводнике
- Двойной клик по файлу

**Способ 2 (через адресную строку):**
```
file:///C:/Users/SNHIM/Desktop/VCd01/frontend/index.html
```

---

## Как это работает

1. Пользователь вводит **нишу/отрасль** и опционально **регион/рынок**.
2. Backend отправляет запрос в локальную Ollama API (`http://localhost:11434/api/chat`).
3. Ollama генерирует **план исследования (5 шагов) в формате JSON**.
4. Для каждого шага формируется промпт и отправляется в Ollama.
5. Результаты по шагам собираются как **лог исследования**.
6. Все промежуточные результаты отправляются в Ollama для построения **финального отчёта с 3–5 бизнес-идеями**.
7. Frontend:
   - Показывает выполнение шагов в реальном времени
   - Отображает финальный отчёт в Markdown-формате
   - Позволяет скопировать результат

---

## Проверка работоспособности

### Тест 1: Проверка здоровья сервера

Откройте в браузере:
```
http://127.0.0.1:8000/health
```

Должно вернуть:
```json
{
  "status": "ok",
  "ollama": true
}
```

### Тест 2: Генерация идей через интерфейс

1. Откройте `frontend/index.html`
2. Введите:
   - **Ниша:** кофейни
   - **Регион:** Москва
3. Нажмите "Сгенерировать идеи"
4. Дождитесь результата (1-3 минуты)

---

## Выбор модели

Вы можете изменить модель в файле `backend/main.py`:
```python
# Лёгкая и быстрая модель (по умолчанию)
ollama = OllamaClient(model="gemma3:1b")

# Более точная модель (требует больше ресурсов)
ollama = OllamaClient(model="deepseek-r1:8b")

# Самая мощная модель (если позволяет железо)
ollama = OllamaClient(model="gemma3:12b")
```

**Сравнение моделей:**

| Модель | Размер | Скорость | Точность | RAM |
|--------|--------|----------|----------|-----|
| gemma3:1b | ~1 GB | Очень быстро | Низкая | ~2 GB |
| gemma3:7b | ~4 GB | Средне | Хорошая | ~6 GB |
| deepseek-r1:8b | ~5 GB | Средне | Отличная | ~8 GB |
| gemma3:12b | ~7 GB | Медленно | Отличная | ~12 GB |

---

## Преимущества локального решения

✅ **Полностью бесплатно** — никаких API-ключей и подписок  
✅ **Работает без интернета** — все данные обрабатываются локально  
✅ **Конфиденциальность** — данные не покидают ваш компьютер  
✅ **Неограниченное использование** — никаких лимитов на запросы  
✅ **Настраиваемость** — можно менять модели и промпты  

---

## Устранение проблем

### Ошибка: "Не удалось подключиться к Ollama"

**Решение:**
1. Убедитесь, что Ollama запущена: `ollama run gemma3:1b`
2. Проверьте настройки: Settings → "Expose Ollama to the network" (галочка)
3. Перезапустите Ollama

### Ошибка: "Ollama не ответила вовремя (timeout)"

**Решение:**
1. Используйте более лёгкую модель (`gemma3:1b`)
2. Увеличьте timeout в `backend/ollama_client.py`:
```python
   timeout=300  # 5 минут
```

### Модель даёт неточные ответы

**Решение:**
1. Установите более мощную модель: `ollama run deepseek-r1:8b`
2. Измените модель в `backend/main.py`

---

## Дальнейшее развитие

- [ ] Добавить Docker-контейнер для backend
- [ ] Реализовать сохранение истории исследований
- [ ] Добавить экспорт результатов в PDF/DOCX
- [ ] Интегрировать веб-поиск для более актуальных данных
- [ ] Добавить выбор модели через интерфейс

---

## Авторы

Проект создан на основе урока по работе с локальными LLM через Ollama.

**Технологии:**
- Python 3.14
- FastAPI
- Ollama (gemma3:1b)
- Vanilla JavaScript